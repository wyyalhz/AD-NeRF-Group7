
由于这是一个2021年的项目，很多依赖包如今已不再支持，按照作者提供的environment.yml下载会有很多异常，下面分享在25年成功复现该项目的经验。

项目源码地址：https://github.com/YudongGuo/AD-NeRF

运行环境：wsl2 + Ubuntu 22.04 + VS Code

## 1.虚拟环境创建

environment.yml 里的很多依赖版本都是老古董了，直接按项目Readme用conda装会出现conda找不到某些已不再支持的包，如果尝试去掉版本号后再安装，conda 大概率会因陷入极其庞大的版本适配计算过程而卡死（再开一个 wsl 终端按 top，发现 conda 一直占用100%cpu）。就算把那些装不了的包的改用pip强行安装好这一套老的依赖，也大概率会在后面编译 pytorch3D 的时候陷入版本地狱。

~~我干脆直接选择到 ~~Python 3.10 + Torch 2.4.1 (cu118) + PyTorch3D 0.7.8~~

沃日，上面那个选择害惨我了。虽然cuda向下兼容，但50系显卡是sm120架构，必须用 pytorch 2.9才行。数据预处理中用到pytorch3D，但是pytorch3D目前只支持到pytorch2.4.1，也就是除非你自己编译pytorch，不然无法在50系显卡上用pytorch3D。

可以用 nvcc --version 查看自己的cuda版本，然后上[pytorch官网](https://pytorch.org/)复制下载命令，对于50系显卡，我们不得不创建3个虚拟环境：

- adnerf_ds: python 3.7 + tensorflow 1.15，只跑数据预处理的step0，把.wav变成.npy
- adnerf_pre：python 3.10 + pytorch 2.4.1cpu + pytorch3D 0.7.7，跑数据预处理剩余部分
- adnerf：python 3.10 + pytorch 2.9.1cuda，训练模型和渲染

如果是40系显卡，只需要创建adnerf_ds和adnerf即可。

### 1.1 adnerf_ds

### 1.1.1 安装python和tensorflow

```bash
conda create -n adnerf_ds python=3.7 -y
conda activate adnerf_ds

# 安装 TF1.15 + 一个兼容 numpy 版本（经典组合）
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple "tensorflow==1.15.0" "numpy==1.19.5"
```

### 1.1.2 下载deepspeech模型

(模型官网)[https://github.com/osmr/deepspeech_features/releases]

```bash
cp /mnt/e/BIT/a3Junior/SpeechRecognitionAndSynthesis/deepspeech-0_1_0-b90017e8.pb/deepspeech-0_1_0-b90017e8.pb ~/.tensorflow/models/deepspeech-0_1_0-b90017e8.pb
```

### 1.1.3 降级 protobuf

```bash
python - << 'EOF'
import google.protobuf
print("protobuf version:", google.protobuf.__version__)
EOF

pip install -i https://pypi.tuna.tsinghua.edu.cn/simple "protobuf==3.20.3" --force-reinstall
```

### 1.2 cpu only环境

```bash
# 新建环境
conda create -n adnerf_pre python=3.10 -y
conda activate adnerf_pre
```

#### 1.2.1 wsl2修改proxy

为了让wsl2走windows上的网络代理，从而更方便地从GitHub上clone源码和下载依赖。方法参考b站教程[wsl2修改proxy](https://www.bilibili.com/video/BV1hm4y187Ps)。打开clash，在ALLOW LAN那行点击nerwork interfaces图标查看WLAN地址，然后打开WSL输入：

```bash
# 根据clash ALLOW LAN里看到的具体端口写，要把ALLOW LAN和TUN MODE都打开
export http_proxy='http://10.194.12.222:7890'
export https_proxy='http://10.194.12.222:7890'
export all_proxy='socks5://10.194.12.222:7890'
export ALL_PROXY='socks5://10.194.12.222:7890'

# 测试一下
curl -I https://www.google.com
curl -I https://github.com
```

#### 1.2.2 安装pytorch和pytorch3D

去[pytorch官网](https://pytorch.org/get-started/previous-versions/)找到对应版本安装命令后直接复制运行；对于pytorch3d建议本地克隆安装，详情可以见[pytorch3d官网](https://github.com/facebookresearch/pytorch3d/blob/main/INSTALL.md)，先安装相关依赖，然后clone到本地后进入文件夹安装，安装时记得关闭隔离（pip install -e .（带 PEP 517/pyproject 的项目）默认会在一个临时的“隔离”构建环境里运行 setup.py / pyproject 的 hooks；这个临时环境里只有构建依赖（pip 临时装的那些），不会继承你当前 env 已有的包；pytorch3d 的 setup.py 里一上来就 import torch；由于“隔离环境”里没装 torch → ModuleNotFoundError: No module named 'torch'）

```bash
pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu128

pip install iopath fvcore pybind11

git clone https://github.com/facebookresearch/pytorch3d.git
cd pytorch3d && pip install -e . --no-build-isolation #关闭隔离
```

都装完后可以进入python环境import一下看看是否成功。其他需要安装的依赖：（手测）

```bash
pip install scipy 
pip install opencv-python face_alignment scikit-learn resampy pandas python_speech_features tensorflow natsort configargparse
```

### 1.3 gpu环境

```bash
# 新建环境
conda create -n adnerf python=3.10 -y
conda activate adnerf

# 安装pytorch
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu128
```

安装好后确认一下：

```bash
python - << 'EOF'
import torch

print("=== PyTorch info ===")
print("torch version:", torch.__version__)
print("cuda available:", torch.cuda.is_available())

if torch.cuda.is_available():
    print("cuda device count:", torch.cuda.device_count())
    print("current device index:", torch.cuda.current_device())
    print("current device name:", torch.cuda.get_device_name(0))

    x = torch.randn(3, 3, device="cuda")
    print("tensor device:", x.device)
else:
    print("!!! CUDA not available in this env.")
EOF
```

再安装一些必要的依赖。由于没有用作者的environment.yml，肯定有漏装的包，后续运行时缺啥补啥就行了。

```bash
pip install numpy scipy tqdm matplotlib \
            opencv-python imageio imageio-ffmpeg \
            configargparse tensorboardX natsort
pip install face-alignment
```

最后下载项目所需的模型，访问 [Basel Face Model 2019 官网](https://faces.dmi.unibas.ch/bfm/main.php?nav=1-1-0&id=details)，根据提示填好信息，从邮箱跳转下载页面完成验证后下载模型，解压得到01_MorphableModel.mat，放在 data_util/face_tracking/3DMM/ 中；然后进入 data_util/face_tracking 目录并运行：

```bash
python convert_BFM.py
```

注意，是放在 wsl 里，别跟Windows搞混了。wsl2 的 /mnt 目录下挂载了Windows的文件系统。也要注意Linux和Windows文件路径的斜杠是相反的

```bash
cp /mnt/e/BIT/a3Junior/SpeechRecognitionAndSynthesis/BaselFaceModel/PublicMM1/01_MorphableModel.mat ./3DMM/
cp /mnt/c/Users/GU/Downloads/trainval_dataset.npy 
```

至此先决条件均已满足。


## 2. 按照Readme一路训练

### 2.1 数据预处理：

作者是这么给的，但我们不用

```bash
bash process_data.sh Obama
```

我们改用分分步执行。




### 先activate adnerf_ds

可能会提示缺少一些依赖，缺啥补啥，然后执行step 0，完成后在$id/目录下生成.npy文件

```bash
# 手测缺的依赖
pip install opencv-python face_alignment scikit-learn resampy pandas python_speech_features tensorflow natsort configargparse

python data_util/process_data.py --id Obama --step 0
```

### 再activate adnerf_pre

执行step1~7

```bash
python data_util/process_data.py --id Obama --step 1
# 后面省略
```

1.face_aligment依赖版本问题：新的face_alignment已经没有_2D这个枚举值了，改名叫TWO_D，导致valid——image_ids变成空列表。我们进入data_util/process_data.py，搜索._2D，改成.TWO_D（95行）

2.AssertionError: Torch not compiled with CUDA enabled

**这个代码不规范的一点在于，像.cuda()这种重复语句，应该定义一个device，定义好用gpu或者cpu后在代码中直接to(device)，而这个项目没有这么做。所以各个文件中都有很多.cuda()，最好让copilot都改一遍。**

我目前改过的：process_data.py，data_loader.py，test.py，face_tracker.py，facemodel.py

face_aligment默认走gpu，需要手动约束只走cpu。在process_data.py中，在fa = face_alignment.FaceAlignment处手动加一个device约束

```python
fa = face_alignment.FaceAlignment(
    face_alignment.LandmarksType._2D,
    flip_input=False,
    device='cpu')
```

在data_loader.py和face_tracker.py中，把.cuda()全部删掉，'cuda:0'改成'cpu'

在test.py中修改evaluate函数：

```python
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

    # net = BiSeNet(n_classes=n_classes)
    # net.cuda()
    net = BiSeNet(n_classes=n_classes).to(device)
    # 2. 加 map_location，避免用 CPU 版 PyTorch 时去找 CUDA
    net.load_state_dict(torch.load(cp, map_location=device))
    net.eval()

    
    # img = img.cuda()
    # 3. 原来的 img.cuda() 改成走同一个 device
    img = img.to(device)
```

4.TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0.

这套 TensorFlow / CUDA 是几年前的，它根本不认识 RTX 5060 Ti 这个算力 12.0 的 GPU，所以 TF 想用 GPU 跑 deepspeech 的时候直接炸了。我们让TF只用cpu：修改 deepspeech_features.py，45行左右，把

```python
import tensorflow as tf
import os

with tf.compat.v1.Session(graph=graph) as sess:
```

改成

```python
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1" 

import tensorflow as tf
tf.compat.v1.disable_eager_execution()

config = tf.compat.v1.ConfigProto(device_count={'GPU': 0})
with tf.compat.v1.Session(graph=graph, config=config) as sess:
```

5.ModuleNotFoundError: No module named 'numpy.lib.function_base'

这份代码是按 老版本 NumPy 写的，直接从 numpy.lib.function_base 里面 import quantile。我现在装的是 NumPy 2.0+，内部目录结构变了，numpy.lib.function_base 这个路径直接没了，所以就 ModuleNotFoundError。numpy.core.numeric 也是内部模块，NumPy 新版已经不想你这么用，所以给了个 DeprecationWarning。

打开data_util/face_tracking/face_tracker.py，把文件开头

```python
from numpy.core.numeric import require
from numpy.lib.function_base import quantile
```

改成

```python
import numpy as np

require = np.require
quantile = np.quantile
```

### 整个预处理过程解读

根据 process_data.py，整个预处理是这么分步骤的（--step 参数）：

- Step 0：从视频抽 aud.wav + Deepspeech 特征

- Step 1：从 vids/Obama.mp4 抽帧到dataset/Obama/ori_imgs/0.jpg, 1.jpg, ...

- Step 2：做人脸关键点（landmarks），用 face_alignment 在 CPU 上跑，对每个 *.jpg 生成一个 *.lms：dataset/Obama/ori_imgs/0.lms …然后脚本不管 step 值是多少，都会做一件事：扫描 ori_imgs 目录，把所有存在 .lms 的帧记进 valid_img_ids
- 
- Step 3：face parsing，实际是调用：

```bash
python data_util/face_parsing/test.py \
  --respath=dataset/Obama/parsing \
  --imgpath=dataset/Obama/ori_imgs
```

输出：dataset/Obama/parsing/0.png, 1.png, ...

- Step 4：从多帧 parsing 结果里，估一个“纯背景图” bc.jpg，输入：parsing/*.png + ori_imgs/*.jpg，输出：dataset/Obama/bc.jpg

- Step 5：生成训练用图像：com_imgs/*.jpg：背景换成 bc.jpg 的完整人物，head_imgs/*.jpg：只保留头部区域

- Step 6：估计 head pose，调用 python data_util/face_tracking/face_tracker.py ...，里面会读 .lms，算姿态，最后存一个dataset/Obama/track_params.pt

- Step 7：读 track_params.pt，生成transforms_train.json，transforms_val.json，一堆 Nerf config txt

## 3.使用预训练模型

由于本人只能用cpu，在数据预处理step7过于龟速，于是直接跳过剩余预处理步骤，直接使用作者给的预训练模型。把那堆.txt和.json粘贴到\Obama\下，然后在\Obama\下新建文件夹logs\，再在logs\里新建Obama_com和Obama_head，最后把两个tar包（也就是训练参数）分别粘贴进这两个文件夹，就完美跳过数据预处理直接开始训练模型了，接下来按作者给的训练命令进行训练，先训练head，训练完后把最新_head.tar粘贴进_com，在_com的训练中会同时训练head和body。记得在两个run_nerf.py中找一下N_iters，把训练轮次改成你需要的（因为作者给的预训练模型body是600000，而默认是400000，所以你不改的话没法在预训练模型基础上继续训练）

```bash
python NeRFs/HeadNeRF/run_nerf.py --config dataset/$id/HeadNeRF_config.txt

python NeRFs/TorsoNeRF/run_nerf.py --config dataset/$id/TorsoNeRF_config.txt
```

## 4.运行AD-NeRF进行渲染

```bash
# 使用音频输入重建原始视频：
python NeRFs/TorsoNeRF/run_nerf.py --config dataset/$id/TorsoNeRFTest_config.txt --aud_file=dataset/$id/aud.npy --test_size=300

# 使用另一个音频输入驱动目标人物
python NeRFs/TorsoNeRF/run_nerf.py --config dataset/$id/TorsoNeRFTest_config.txt --aud_file=${deepspeechfile.npy} --test_size=-1
```

## 5.用ffmpeg将音频和视频合成

NeRF输出的.avi文件一般在\Obama_com\test_aud_rst下，可以用命令找一下啊
```bash
find dataset/Obama -maxdepth 5 \( -name "*.avi" -o -name "*.jpg" -o -name "*.mp4" \) -mmin -10
```

